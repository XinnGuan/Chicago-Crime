---
title: "Chicago Crime"
author: "Xin Guan, Vera Hudak, Yuqi Zhang"
date: "2023-11-24"
output:
  pdf_document: default
  html_document: default
---
```{r}
# Packages required
library(lubridate)
library(dplyr)
library(magrittr)
library(tidyverse)
library(ggplot2)
library(caret)
library(pROC)
library(MLmetrics)
library(ROSE)
library(knitr)
library(sf)
library(spatstat)
```

## Data Processing

```{r}
# Read data
df <- read.csv("OriginalData.csv")
```

### Processing variable 'Date'
```{r}
data <- df %>% select(ID,Date, Primary.Type, Location.Description, Arrest, District)
data %<>% filter(!Primary.Type %in% c("NON-CRIMINAL", "OTHER NARCOTIC VIOLATION", "PUBLIC INDECENCY", "HUMAN TRAFFICKING", "OBSCENITY"))
data$Date <- parse_datetime(data$Date, format = "%m/%d/%Y %I:%M:%S %p")
data %<>%
  mutate(Date = as.POSIXct(Date, format = "%m/%d/%Y %I:%M:%S %p"),
         time = format(as.POSIXct(Date), format = "%H"),
         date = as.Date(Date),
         month = format(as.Date(date, format = "%m/%d/%Y"), "%m"),
         DayofYear = yday(date),
         weekday = weekdays(date))
```



### Processing missing data
```{r}
colSums(is.na(data))
```
We can see that the number of missing values for the variables `X.Coordinate`, `Y.Coordinate`, `Latitude`, and `Longtitude` are exactly the same, we can deduce that the coordinates are calculated from the latitude and longitude, so we don't need to include both pairs of location information. Also since the number of missing value is small compare to the number of data size, we will just eliminate the rows with missing values.
```{r}
data %<>%  na.omit()
```

## Expalnatory Data Analysis
```{r}
data$Primary.Type <- as.factor(data$Primary.Type)
data$time <- as.numeric(data$time)
data$District <- as.factor(data$District)
data$Weekday <- as.factor(data$Weekday)

data <- data %>% select(ID, Primary.Type, Arrest, District, time, DayOfYear, Weekday) %>% drop_na()
```
```{r}
# Investigate of variables
df_vis <- df

df_vis$newDate <- mdy_hms(df_vis$Date)
df_vis$WeekDay <- weekdays(df_vis$newDate)
df_vis$DayOfMonth <- day(df_vis$newDate)
df_vis$DayOfYear <- yday(df_vis$newDate)
df_vis$Month <- month(df_vis$newDate, label = TRUE, abbr = FALSE)
df_vis$TimeOfDay <- cut(
  hour(df_vis$newDate),
  breaks= c(-Inf, 5, 12, 17, 20, Inf),
  labels = c("Night", "Early Morning", "Morning", "Afternoon", "Evening"),
  include.lowest = TRUE
)

# Detailed Time
# This format of `Time` variable contains information of hour and minute only
df_vis$newDate <- as.POSIXct(df_vis$newDate, format = "%H:%M:%S")
df_vis$Hour <- hour(df_vis$newDate)
df_vis$Minute <- minute(df_vis$newDate)/60*10
df_vis$Time <- df_vis$Hour + df_vis$Minute/60


# ----------------------------------------------------------------------------
# Components of Dates
# ----------------------------------------------------------------------------
# Weekdays
arrests_by_weekday <- df_vis %>%
  group_by(WeekDay) %>%
  summarise(num_arrests = n())

ggplot(arrests_by_weekday, aes(x = WeekDay, y = num_arrests, fill = WeekDay)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Arrests by Weekday",
       x = "Weekday",
       y = "Number of Arrests") +
  theme_minimal()

# Month
arrests_by_Month <- df_vis %>%
  group_by(Month) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = Month, fill = Month)) +
  geom_bar() +
  labs(title = "Number of Arrests by Month",
       x = "Month",
       y = "Number of Arrests") +
  theme_minimal()

# DayOfMonth
arrests_by_DayOfMonth <- df_vis %>%
  group_by(DayOfMonth) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = DayOfMonth, fill = DayOfMonth)) +
  geom_bar() +
  labs(title = "Number of Arrests by DayOfMonth",
       x = "DayOfMonth",
       y = "Number of Arrests") +
  theme_minimal()

# DayOfYear
arrests_by_DayOfYear <- df_vis %>%
  group_by(DayOfYear) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = DayOfYear, fill = DayOfYear)) +
  geom_bar() +
  labs(title = "Number of Arrests by DayOfYear",
       x = "DayOfYear",
       y = "Number of Arrests") +
  theme_minimal()


# TimeOfDay
arrests_by_TimeOfDay <- df_vis %>%
  group_by(TimeOfDay) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = TimeOfDay, fill = TimeOfDay)) +
  geom_bar() +
  labs(title = "Number of Arrests by TimeOfDay",
       x = "TimeOfDay",
       y = "Number of Arrests") +
  theme_minimal()

# hours
arrests_by_hour <- df_vis %>%
  group_by(Hour) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = Hour, fill = Hour)) +
  geom_bar() +
  labs(title = "Number of Arrests by Hour",
       x = "Hour",
       y = "Number of Arrests") +
  theme_minimal()

# District
arrests_by_dist <- df_vis %>%
  group_by(District) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = District, fill = District)) +
  geom_bar() +
  labs(title = "Number of Arrests by District",
       x = "District",
       y = "Number of Arrests") +
  theme_minimal()


# ----------------------------------------------------------------------------
# Relations between `Primary.Type` and `TimeOfDay` in a specific `Weekday`
# ----------------------------------------------------------------------------
# Choose a specific weekday (e.g., "Monday")
selected_weekday <- "Monday"
# Filter the data for the selected weekday
filtered_data <- df_vis %>% filter(WeekDay == selected_weekday)

# Summary table with the count of crimes for each combination of `Primary.Type` and `TimeOfDay`
crime_time_counts <- filtered_data %>%
  group_by(Primary.Type, TimeOfDay) %>%
  summarise(Count = n())

ggplot(crime_time_counts, aes(x = TimeOfDay, y = Primary.Type, fill = Count)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(title = paste("Primary.Type and TimeOfDay Relationship on", selected_weekday),
       x = "Time of the Day",
       y = "Primary.Type",
       fill = "Crime Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right")

```






## Feature Selection
Our first task is to predict whether arrest or not given time, location, and the crime type.
```{r}
data %<>% filter(District != "31")
sum(data$District=="31")
```




## Define Cross-validation
```{r}
# Define a 5-fold cross validation
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = twoClassSummary, classProbs = TRUE)
```





## Imbalance Data Experiments with a Baseline Model

```{r}
# Separate positive and negative classes
true_class <- data %>% filter(Arrest == "true")
false_class <- data %>% filter(Arrest == "false")

# Set the desired ratio of positive to negative samples
desired_positive_ratio <- 0.05

# Calculate the number of positive samples needed for downsampling
num_true_samples <- 0.05*nrow(false_class) / (1 - desired_positive_ratio)

# Randomly sample positive samples
true_class_downsampled <- true_class %>% sample_n(num_true_samples, replace = FALSE, seed = 42)

# Combine positive and downsampled negative samples
downsampled_dataset <- bind_rows(false_class, true_class_downsampled)

# Shuffle the dataset to mix positive and negative samples
set.seed(42)
downsampled_dataset <- downsampled_dataset[sample(nrow(downsampled_dataset)), ]

sum(downsampled_dataset[,"Arrest"]=="true")/dim(df)[1]
```

```{r}
#use 80% of dataset as training set and 20% as test set 
variables_to_convert <- c("Arrest", "Primary.Type", "weekday")
imbalanced_train <- downsampled_dataset %>% dplyr::sample_frac(0.80)
imbalanced_train[, variables_to_convert] <- lapply(imbalanced_train[, variables_to_convert], as.factor)
imbalanced_test  <- dplyr::anti_join(downsampled_dataset, imbalanced_train, by = 'ID')
imbalanced_test[, variables_to_convert] <- lapply(imbalanced_test[, variables_to_convert], as.factor)
imbalanced_train %<>% select(-"ID")
imbalanced_test %<>% select(-"ID")
```

```{r}
base_m <- train(Arrest ~ District + DayofYear + time + 
    Primary.Type, data = imbalanced_train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
```

```{r}
pred <- predict(base_m, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
```

```{r}
binary_predictions <- ifelse(pred[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions), imbalanced_test$Arrest,
                mode = "everything",
                positive="true")
```

```{r}
#over sampling
train_balanced_over <- ovun.sample(Arrest ~ District + DayofYear + time + 
    Primary.Type, data = imbalanced_train, method = "over")$data
train_balanced_over$Arrest <- as.factor(train_balanced_over$Arrest)
train_balanced_under <- ovun.sample(Arrest ~ District + DayofYear + time + 
    Primary.Type, data = imbalanced_train, method = "under")$data
train_balanced_under$Arrest <- as.factor(train_balanced_under$Arrest)
#train.rose <- ROSE(Arrest ~ District + DayofYear + time + Primary.Type, data = imbalanced_train)$data
#train.rose$Arrest <- as.factor(train.rose$Arrest)
#table(train.rose$Arrest)
table(train_balanced_over$Arrest)
table(train_balanced_under$Arrest)
```

```{r}
base_m_under <- train(Arrest ~District + DayofYear + time + Primary.Type, data = train_balanced_under, method = "glm", family = binomial, trControl = ctrl, metric = "ROC")
base_m_over <- train(Arrest ~ District + DayofYear + time + Primary.Type, data = train_balanced_over, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
#base_m_rose <- train(Arrest ~ District + DayOfYear + Hour + Primary.Type, data = train.rose, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
```

```{r}
pred_under <- predict(base_m_under, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
pred_over <- predict(base_m_over, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
#pred_rose <- predict(base_m_rose, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
```

```{r}
# Create a ROC curve object for each prediction
roc_curve <- pROC::roc(imbalanced_test$Arrest, pred[,2])
#the roc function name is used in several packages, `pROC::roc` is used to ensure the roc function is from pROC package.
roc_curve_under <- pROC::roc(imbalanced_test$Arrest, pred_under[,2])
roc_curve_over <- pROC::roc(imbalanced_test$Arrest, pred_over[,2])
#roc_curve_rose <- pROC::roc(imbalanced_test$Arrest, pred_rose[,2])

# Plot the first ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve")

# Add AUC to the plot for the first curve
auc_value <- pROC::auc(roc_curve)
text(0.8, 0.2, paste("AUC =", round(auc_value, 3)), col = "blue", cex = 1.2)

# Add the other ROC curves to the plot
lines(roc_curve_under, col = "red", lwd = 2)
lines(roc_curve_over, col = "green", lwd = 2)
#lines(roc_curve_rose, col = "purple", lwd = 2)

# Add AUC values for the other curves
text(0.8, 0.15, paste("AUC =", round(pROC::auc(roc_curve_under), 3)), col = "red", cex = 1.2)
text(0.8, 0.1, paste("AUC =", round(pROC::auc(roc_curve_over), 3)), col = "green", cex = 1.2)
#text(0.8, 0.05, paste("AUC =", round(pROC::auc(roc_curve_rose), 3)), col = "purple", cex = 1.2)

legend("bottomright", legend = c("Original", "Under-sampled", "Over-sampled"),
       col = c("blue", "red", "green"), lty = 1, lwd = 2)
```
Given the current highly imbalanced nature of the data, the effectiveness of comparing models using the AUC curve diminishes. It's advisable to explore alternative evaluation metrics. The provided code below computes the F-$\beta$ score, an generalization of the F-1 score, the F-1 score is included in the output of `confusionMatrix()` function. The parameter $\beta \geq 0$ is the weight assigned to recall, in the case, we should choose $\beta>1$, so we prioritize the recall.

## F-beta Score
```{r}
f_beta_score <- function(X, y, model, beta, threshold=0.5) {
  predictions <- predict(model, X, type = "prob")[,2]
  predicted_classes <- ifelse(predictions > threshold, "true", "false")
  predicted_classes <- as.factor(predicted_classes)
  pr <- confusionMatrix(data = predicted_classes, reference = y, 
                        mode = "everything", positive="true")

  precision <- as.numeric(pr$byClass["Precision"])
  recall <- as.numeric(pr$byClass["Recall"])

  f_beta <- ((1 + beta^2) * precision * recall) / (beta^2 * precision + recall)
  return(f_beta)
}
```

```{r}
f2 <- f_beta_score(X=imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], y=imbalanced_test$Arrest,model=base_m,beta=5)
f2_under <- f_beta_score(X=imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], y=imbalanced_test$Arrest,model=base_m_under,beta=5)
f2_over <- f_beta_score(X=imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], y=imbalanced_test$Arrest,model=base_m_over,beta=5)
print(c(f2,f2_under,f2_over))
```





## Train Test Split

```{r}
#use 80% of dataset as training set and 20% as test set 
variables_to_convert <- c("Arrest", "Primary.Type", "WeekDay")
train <- Binary_pred_df %>% dplyr::sample_frac(0.80)
train[, variables_to_convert] <- lapply(train[, variables_to_convert], as.factor)
test  <- dplyr::anti_join(Binary_pred_df, train, by = 'ID')
test[, variables_to_convert] <- lapply(test[, variables_to_convert], as.factor)
train %<>% select(-"ID")
test %<>% select(-"ID")
```

## Predict Probability of Arrest
### Logistic Regression Model

```{r}
# Define a 5-fold cross validation
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = twoClassSummary, classProbs = TRUE)
```

 - base

```{r}
base_m <- train(Arrest ~ District + DayOfYear + time + Primary.Type, 
                data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
```

```{r}
pred <- predict(base_m, test[,c("District", "DayOfYear", "time", "Primary.Type")], type = "prob")
```

```{r}
binary_predictions <- ifelse(pred[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions), test$Arrest,
                mode = "everything",
                positive="true")
roc_curve <- pROC::roc(test$Arrest, pred[,2],levels = c("true", "false"),direction = ">")
plot(roc_curve, main = "ROC Curve for Baseline Model", col = "blue", lwd = 2)

auc_curve <- pROC::auc(roc_curve)
text(0.8, 0.2, paste("AUC =", round(auc_curve, 3)), col = "blue", cex = 1.2)
```

 - improvement of baseline 1

```{r}
logistic_im_1 <- train(Arrest ~ District + DayOfYear + Weekday + time + Primary.Type, 
                       data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")

# Cannot use `Time` - result in multicollineararity
# DayOfMonth and Month 

pred_im_1 <- predict(logistic_im_1, test[,c("District", "DayOfYear", "Weekday", "time", "Primary.Type")], type = "prob")

binary_predictions_im_1 <- ifelse(pred_im_1[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions_im_1), test$Arrest,
                mode = "everything",
                positive="true")

roc_1 <- pROC::roc(test$Arrest, pred_im_1[,2], levels = c("true", "false"),direction = ">")
plot(roc_1, main = "ROC Curve", col = "blue", lwd = 2)

auc_1 <- pROC::auc(roc_1)
text(0.8, 0.2, paste("AUC =", round(auc_1, 3)), col = "blue", cex = 1.2)

```

 - improvement of baseline 2

```{r}
train$time_t <- asin(train$time/24)
test$time_t <- asin(test$time/24)

logistic_im_2 <- train(Arrest ~ District + DayOfYear + Weekday + time_t + Primary.Type, 
                       data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")

# Cannot use `Time` - result in multicollineararity

pred_im_2 <- predict(logistic_im_2, test[,c("District", "DayOfYear", "Weekday", "time_t", "Primary.Type")], type = "prob")

binary_predictions_im_2 <- ifelse(pred_im_2[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions_im_2), test$Arrest,
                mode = "everything",
                positive="true")

roc_2 <- pROC::roc(test$Arrest, pred_im_2[,2], levels = c("true", "false"),direction = ">")
plot(roc_2, main = "ROC Curve", col = "blue", lwd = 2)

auc_2 <- pROC::auc(roc_2)
text(0.8, 0.2, paste("AUC =", round(auc_2, 3)), col = "blue", cex = 1.2)
```



## Support Vector Machine






## Predict Number of Cases withing Radius of 5 km

```{r}
crime_sf <- st_as_sf(Crime_data, coords = c("X.Coordinate", "Y.Coordinate"))
#Convert your data to an sf object
crime_pp <- as.ppp(crime_sf)
#Create a point pattern object
```

```{r}
center <- c(longitude = -87.68737, latitude = 41.80292)
# Specify the radius in kilometers
radius_km <- 5

# Convert radius to degrees (approximate)
radius_deg <- radius_km / 111.32  # 1 degree is approximately 111.32 kilometers

# Create a bounding box around the center coordinate
bbox <- st_bbox(st_buffer(st_point(c(center["longitude"], center["latitude"])), dist = radius_deg))

# Subset points within the bounding box
points_within_bbox <- st_within(crime_sf, bbox)

# Calculate distances from the center and filter points within the radius
distances <- st_distance(st_geometry(crime_sf), st_point(c(center["longitude"], center["latitude"])))
points_within_radius <- crime_sf[distances <= radius_deg, ]

# Print the result or use it as needed
print(points_within_radius)
```

```{r}
# Assuming you have a data frame named crime_data with columns Latitude and Longitude
crime_sf <- st_as_sf(Crime_data, coords = c("Longitude", "Latitude"))

# Specify the center coordinate (latitude and longitude)
center <- c(longitude = -87.68737, latitude = 41.80292)

# Specify the radius in kilometers
radius_km <- 5

# Convert radius to meters for more accurate distance calculations
radius_meters <- radius_km * 1000

# Create a circular buffer around the center point
buffer <- st_buffer(st_point(c(center["longitude"], center["latitude"])), dist = radius_meters)

# Subset points within the buffer
points_within_buffer <- st_intersection(crime_sf, buffer)

# Print
```


