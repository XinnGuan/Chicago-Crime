---
title: "Chicago Crime"
author: "Xin Guan, Vera Hudak, Yuqi Zhang"
date: "2023-11-24"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Packages required
library(lubridate)
library(dplyr)
library(magrittr)
library(tidyverse)
library(ggplot2)
library(caret)
library(pROC)
library(MLmetrics)
library(ROSE)
library(knitr)
#library(sf)
library(spatstat)

library(ggplot2)
library(tidyr)
library(dplyr)
library(readr)
library(e1071)
library(doParallel)
library(mltools)
library(caret)
library(kernlab)
library(ROCR)
library(data.table)

library(DescTools)
library(scales)

library(geosphere)
```

## Data Processing

```{r}
# Read data
df <- read.csv("OriginalData.csv")
```

### Processing variable 'Date'

```{r}
data <- df %>% select(ID,Date, Primary.Type, Location.Description, Arrest, District)
data %<>% filter(!Primary.Type %in% c("NON-CRIMINAL", "OTHER NARCOTIC VIOLATION", "PUBLIC INDECENCY", "HUMAN TRAFFICKING", "OBSCENITY"))
data$Date <- parse_datetime(data$Date, format = "%m/%d/%Y %I:%M:%S %p")
data %<>%
  mutate(Date = as.POSIXct(Date, format = "%m/%d/%Y %I:%M:%S %p"),
         time = format(as.POSIXct(Date), format = "%H"),
         date = as.Date(Date),
         month = format(as.Date(date, format = "%m/%d/%Y"), "%m"),
         DayofYear = yday(date),
         weekday = weekdays(date))
```

### Processing missing data

```{r}
colSums(is.na(data))
```

We can see that the number of missing values for the variables `X.Coordinate`, `Y.Coordinate`, `Latitude`, and `Longtitude` are exactly the same, we can deduce that the coordinates are calculated from the latitude and longitude, so we don't need to include both pairs of location information. Also since the number of missing value is small compare to the number of data size, we will just eliminate the rows with missing values.

```{r}
data %<>%  na.omit()
```

## Expalnatory Data Analysis

```{r}
data$Primary.Type <- as.factor(data$Primary.Type)
data$time <- as.numeric(data$time)
data$District <- as.factor(data$District)
data$weekday <- as.factor(data$weekday)

data <- data %>% select(ID, Primary.Type, Arrest, District, time, DayofYear, weekday) %>% drop_na()
```

```{r}
# Investigate of variables
df_vis <- df

df_vis$newDate <- mdy_hms(df_vis$Date)
df_vis$WeekDay <- weekdays(df_vis$newDate)
df_vis$DayOfMonth <- day(df_vis$newDate)
df_vis$DayOfYear <- yday(df_vis$newDate)
df_vis$Month <- lubridate::month(df_vis$newDate, label = TRUE, abbr = FALSE)
df_vis$TimeOfDay <- cut(
  hour(df_vis$newDate),
  breaks= c(-Inf, 5, 12, 17, 20, Inf),
  labels = c("Night", "Early Morning", "Morning", "Afternoon", "Evening"),
  include.lowest = TRUE
)

# Detailed Time
# This format of `Time` variable contains information of hour and minute only
df_vis$newDate <- as.POSIXct(df_vis$newDate, format = "%H:%M:%S")
df_vis$Hour <- hour(df_vis$newDate)
df_vis$Minute <- minute(df_vis$newDate)/60*10
df_vis$Time <- df_vis$Hour + df_vis$Minute/60


# ----------------------------------------------------------------------------
# Components of Dates
# ----------------------------------------------------------------------------
# Weekdays
arrests_by_weekday <- df_vis %>%
  group_by(WeekDay) %>%
  summarise(num_arrests = n())

ggplot(arrests_by_weekday, aes(x = WeekDay, y = num_arrests, fill = WeekDay)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Arrests by Weekday",
       x = "Weekday",
       y = "Number of Arrests") +
  theme_minimal()

# Month
arrests_by_Month <- df_vis %>%
  group_by(Month) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = Month, fill = Month)) +
  geom_bar() +
  labs(title = "Number of Arrests by Month",
       x = "Month",
       y = "Number of Arrests") +
  theme_minimal()

# DayOfMonth
arrests_by_DayOfMonth <- df_vis %>%
  group_by(DayOfMonth) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = DayOfMonth, fill = DayOfMonth)) +
  geom_bar() +
  labs(title = "Number of Arrests by DayOfMonth",
       x = "DayOfMonth",
       y = "Number of Arrests") +
  theme_minimal()

# DayOfYear
arrests_by_DayOfYear <- df_vis %>%
  group_by(DayOfYear) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = DayOfYear, fill = DayOfYear)) +
  geom_bar() +
  labs(title = "Number of Arrests by DayOfYear",
       x = "DayOfYear",
       y = "Number of Arrests") +
  theme_minimal()


# TimeOfDay
arrests_by_TimeOfDay <- df_vis %>%
  group_by(TimeOfDay) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = TimeOfDay, fill = TimeOfDay)) +
  geom_bar() +
  labs(title = "Number of Arrests by TimeOfDay",
       x = "TimeOfDay",
       y = "Number of Arrests") +
  theme_minimal()

# hours
arrests_by_hour <- df_vis %>%
  group_by(Hour) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = Hour, fill = Hour)) +
  geom_bar() +
  labs(title = "Number of Arrests by Hour",
       x = "Hour",
       y = "Number of Arrests") +
  theme_minimal()

# District
arrests_by_dist <- df_vis %>%
  group_by(District) %>%
  summarise(num_arrests = n())

ggplot(df_vis, aes(x = District, fill = District)) +
  geom_bar() +
  labs(title = "Number of Arrests by District",
       x = "District",
       y = "Number of Arrests") +
  theme_minimal()


# ----------------------------------------------------------------------------
# Relations between `Primary.Type` and `TimeOfDay` in a specific `Weekday`
# ----------------------------------------------------------------------------
# Choose a specific weekday (e.g., "Monday")
selected_weekday <- "Monday"
# Filter the data for the selected weekday
filtered_data <- df_vis %>% filter(WeekDay == selected_weekday)

# Summary table with the count of crimes for each combination of `Primary.Type` and `TimeOfDay`
crime_time_counts <- filtered_data %>%
  group_by(Primary.Type, TimeOfDay) %>%
  summarise(Count = n())

ggplot(crime_time_counts, aes(x = TimeOfDay, y = Primary.Type, fill = Count)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(title = paste("Primary.Type and TimeOfDay Relationship on", selected_weekday),
       x = "Time of the Day",
       y = "Primary.Type",
       fill = "Crime Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right")

```

## Feature Selection

Our first task is to predict whether arrest or not given time, location, and the crime type.

```{r}
data %<>% filter(District != "31")
sum(data$District=="31")
```

## Define Cross-validation

```{r}
# Define a 5-fold cross validation
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = twoClassSummary, classProbs = TRUE)
```

## Imbalance Data Experiments with a Baseline Model

```{r}
set.seed(123)
# Separate positive and negative classes
true_class <- data %>% filter(Arrest == "true")
false_class <- data %>% filter(Arrest == "false")

# Set the desired ratio of positive to negative samples
desired_positive_ratio <- 0.05

# Calculate the number of positive samples needed for downsampling
num_true_samples <- 0.05*nrow(false_class) / (1 - desired_positive_ratio)

# Randomly sample positive samples
true_class_downsampled <- true_class %>% sample_n(num_true_samples, replace = FALSE, seed = 42)

# Combine positive and downsampled negative samples
downsampled_dataset <- bind_rows(false_class, true_class_downsampled)

# Shuffle the dataset to mix positive and negative samples
set.seed(42)
downsampled_dataset <- downsampled_dataset[sample(nrow(downsampled_dataset)), ]

sum(downsampled_dataset[,"Arrest"]=="true")/dim(df)[1]
```

```{r}
set.seed(123)
#use 80% of dataset as training set and 20% as test set 
variables_to_convert <- c("Arrest", "Primary.Type", "weekday")
imbalanced_train <- downsampled_dataset %>% dplyr::sample_frac(0.80)
imbalanced_train[, variables_to_convert] <- lapply(imbalanced_train[, variables_to_convert], as.factor)
imbalanced_test  <- dplyr::anti_join(downsampled_dataset, imbalanced_train, by = 'ID')
imbalanced_test[, variables_to_convert] <- lapply(imbalanced_test[, variables_to_convert], as.factor)
imbalanced_train %<>% select(-"ID")
imbalanced_test %<>% select(-"ID")
```

```{r}
base_m <- train(Arrest ~ District + DayofYear + time + 
    Primary.Type, data = imbalanced_train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
```

```{r}
pred <- predict(base_m, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
```

```{r}
binary_predictions <- ifelse(pred[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions), imbalanced_test$Arrest,
                mode = "everything",
                positive="true")
```

```{r}
set.seed(123)
#over sampling
train_balanced_over <- ovun.sample(Arrest ~ District + DayofYear + time + 
    Primary.Type, data = imbalanced_train, method = "over")$data
train_balanced_over$Arrest <- as.factor(train_balanced_over$Arrest)
train_balanced_under <- ovun.sample(Arrest ~ District + DayofYear + time + 
    Primary.Type, data = imbalanced_train, method = "under")$data
train_balanced_under$Arrest <- as.factor(train_balanced_under$Arrest)
#train.rose <- ROSE(Arrest ~ District + DayofYear + time + Primary.Type, data = imbalanced_train)$data
#train.rose$Arrest <- as.factor(train.rose$Arrest)
#table(train.rose$Arrest)
table(train_balanced_over$Arrest)
table(train_balanced_under$Arrest)
```

```{r}
base_m_under <- train(Arrest ~District + DayofYear + time + Primary.Type, data = train_balanced_under, method = "glm", family = binomial, trControl = ctrl, metric = "ROC")
base_m_over <- train(Arrest ~ District + DayofYear + time + Primary.Type, data = train_balanced_over, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
#base_m_rose <- train(Arrest ~ District + DayOfYear + Hour + Primary.Type, data = train.rose, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
```

```{r}
pred_under <- predict(base_m_under, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
pred_over <- predict(base_m_over, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
#pred_rose <- predict(base_m_rose, imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
```

```{r}
# Create a ROC curve object for each prediction
roc_curve <- pROC::roc(imbalanced_test$Arrest, pred[,2])
#the roc function name is used in several packages, `pROC::roc` is used to ensure the roc function is from pROC package.
roc_curve_under <- pROC::roc(imbalanced_test$Arrest, pred_under[,2])
roc_curve_over <- pROC::roc(imbalanced_test$Arrest, pred_over[,2])
#roc_curve_rose <- pROC::roc(imbalanced_test$Arrest, pred_rose[,2])

# Plot the first ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve")

# Add AUC to the plot for the first curve
auc_value <- pROC::auc(roc_curve)
text(0.8, 0.2, paste("AUC =", round(auc_value, 3)), col = "blue", cex = 1.2)

# Add the other ROC curves to the plot
lines(roc_curve_under, col = "red", lwd = 2)
lines(roc_curve_over, col = "green", lwd = 2)
#lines(roc_curve_rose, col = "purple", lwd = 2)

# Add AUC values for the other curves
text(0.8, 0.15, paste("AUC =", round(pROC::auc(roc_curve_under), 3)), col = "red", cex = 1.2)
text(0.8, 0.1, paste("AUC =", round(pROC::auc(roc_curve_over), 3)), col = "green", cex = 1.2)
#text(0.8, 0.05, paste("AUC =", round(pROC::auc(roc_curve_rose), 3)), col = "purple", cex = 1.2)

legend("bottomright", legend = c("Original", "Under-sampled", "Over-sampled"),
       col = c("blue", "red", "green"), lty = 1, lwd = 2)
```
```{r}
under_predictions <- ifelse(pred_under[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(under_predictions), imbalanced_test$Arrest,
                mode = "everything",
                positive="true")
over_predictions <- ifelse(pred_over[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(over_predictions), imbalanced_test$Arrest,
                mode = "everything",
                positive="true")
```

Given the current highly imbalanced nature of the data, the effectiveness of comparing models using the AUC curve diminishes. It's advisable to explore alternative evaluation metrics. The provided code below computes the F-$\beta$ score, an generalization of the F-1 score, the F-1 score is included in the output of `confusionMatrix()` function. The parameter $\beta \geq 0$ is the weight assigned to recall, in the case, we should choose $\beta>1$, so we prioritize the recall.

## F-beta Score

```{r}
f_beta_score <- function(X, y, model, beta, threshold=0.5) {
  predictions <- predict(model, X, type = "prob")[,2]
  predicted_classes <- ifelse(predictions > threshold, "true", "false")
  predicted_classes <- as.factor(predicted_classes)
  pr <- confusionMatrix(data = predicted_classes, reference = y, 
                        mode = "everything", positive="true")

  precision <- as.numeric(pr$byClass["Precision"])
  recall <- as.numeric(pr$byClass["Recall"])

  f_beta <- ((1 + beta^2) * precision * recall) / (beta^2 * precision + recall)
  return(f_beta)
}
```

```{r}
f2 <- f_beta_score(X=imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], y=imbalanced_test$Arrest,model=base_m,beta=5)
f2_under <- f_beta_score(X=imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], y=imbalanced_test$Arrest,model=base_m_under,beta=5)
f2_over <- f_beta_score(X=imbalanced_test[,c("District", "DayofYear", "time", "Primary.Type")], y=imbalanced_test$Arrest,model=base_m_over,beta=5)
print(c(f2,f2_under,f2_over))
```

## Train Test Split

```{r}
#use 80% of dataset as training set and 20% as test set 
variables_to_convert <- c("Arrest", "Primary.Type", "weekday")
train <- data %>% dplyr::sample_frac(0.80)
train[, variables_to_convert] <- lapply(train[, variables_to_convert], as.factor)
test  <- dplyr::anti_join(data, train, by = 'ID')
test[, variables_to_convert] <- lapply(test[, variables_to_convert], as.factor)
train %<>% select(-"ID")
test %<>% select(-"ID")
```

## Predict Probability of Arrest

### Logistic Regression Model

```{r}
# Define a 5-fold cross validation
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = twoClassSummary, classProbs = TRUE)
```

-   base

```{r}
base_m <- train(Arrest ~ District + DayofYear + time + Primary.Type, 
                data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")
```

```{r}
pred <- predict(base_m, test[,c("District", "DayofYear", "time", "Primary.Type")], type = "prob")
```

```{r}
binary_predictions <- ifelse(pred[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions), test$Arrest,
                mode = "everything",
                positive="true")
roc_curve <- pROC::roc(test$Arrest, pred[,2],levels = c("true", "false"),direction = ">")
plot(roc_curve, main = "ROC Curve for Baseline Model", col = "blue", lwd = 2)

auc_curve <- pROC::auc(roc_curve)
text(0.8, 0.2, paste("AUC =", round(auc_curve, 3)), col = "blue", cex = 1.2)
```

-   improvement of baseline 1

```{r}
logistic_im_1 <- train(Arrest ~ District + DayofYear + weekday + time + Primary.Type, 
                       data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")

# Cannot use `Time` - result in multicollineararity
# DayOfMonth and Month 

pred_im_1 <- predict(logistic_im_1, test[,c("District", "DayofYear", "weekday", "time", "Primary.Type")], type = "prob")

binary_predictions_im_1 <- ifelse(pred_im_1[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions_im_1), test$Arrest,
                mode = "everything",
                positive="true")

roc_1 <- pROC::roc(test$Arrest, pred_im_1[,2], levels = c("true", "false"),direction = ">")
plot(roc_1, main = "ROC Curve", col = "blue", lwd = 2)

auc_1 <- pROC::auc(roc_1)
text(0.8, 0.2, paste("AUC =", round(auc_1, 3)), col = "blue", cex = 1.2)

```

-   improvement of baseline 2

```{r}
train$time_t <- train$time^2
test$time_t <- test$time^2
train$DayofYear_t <- train$DayofYear^2
test$DayofYear_t <- test$DayofYear^2

logistic_im_2 <- train(Arrest ~ District + DayofYear_t + weekday + time_t + Primary.Type, 
                       data = train, method = "glm", family = "binomial", trControl = ctrl, metric = "ROC")

# Cannot use `Time` - result in multicollineararity

pred_im_2 <- predict(logistic_im_2, test[,c("District", "DayofYear_t", "weekday", "time_t", "Primary.Type")], type = "prob")

binary_predictions_im_2 <- ifelse(pred_im_2[,2] >= 0.5, "true", "false")
# create confusion matrix 
confusionMatrix(as.factor(binary_predictions_im_2), test$Arrest,
                mode = "everything",
                positive="true")

roc_2 <- pROC::roc(test$Arrest, pred_im_2[,2], levels = c("true", "false"),direction = ">")
plot(roc_2, main = "ROC Curve", col = "blue", lwd = 2)

auc_2 <- pROC::auc(roc_2)
text(0.8, 0.2, paste("AUC =", round(auc_2, 3)), col = "blue", cex = 1.2)
```

## Support Vector Machine

Next, we use SVM to predict the outcome of a crime (arrest or no arrest) using the variables ``Primary.Type``, ``Disctrict``, ``time``, ``DayofYear`` and ``weekday``. As the relationship between outcome and variables is unclear, we use SVM with radial basis kernel. For SVM, the variables need to be one hot encoded. We also drop the variable ``DayofYear``.

```{r}
# One hot encode data
data_svm <- data %>% select(-c(ID,DayofYear))
data_svm <- as.data.table(data_svm)
data_svm <- one_hot(data_svm)
```

Support Vector Machines (SVMs) face challenges with large datasets due to their cubic time complexity during training, making them computationally expensive as the dataset size increases. Additionally, SVMs often require storing the entire training dataset in memory, which can be impractical for large datasets, leading to reliance on disk storage and slowing down the training process. Furthermore, SVMs exhibit scalability issues, as the optimization problem they solve is quadratic in the number of samples, diminishing their efficiency for large-scale datasets. So when fitting our models, we will use a random sample from our data set, using 3 types of sampling methods.

1. Proportional sampling: this sampling methods generates a sample where the proportion of each classes emulates the original data set.

We take 6% of the data to train our model on, keeping the class proportions as in the original data set and use it to fit the SVM.

```{r}
# Take proportional training sample 
index <- createDataPartition(data_svm$Arrest, p = .06, list = TRUE)
train <- data_svm[index$Resample,]
train %>% dplyr::count(Arrest)
```


```{r}
# CV seeds
seeds <- vector(mode = "list", length = 5)
for(i in 1:5) {
  seeds[[i]] <- sample.int(n = 1000, 3) }

# CV function with 4-fold cross-validation
control <- trainControl(method = 'cv', number = 4, allowParallel = TRUE, seeds = seeds)

```


```{r}
# SVM with proportional sample
cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

svm.model <- train(Arrest ~ ., data = train, trControl = control, method = 'svmRadial', family = binomial(), allowParallel = TRUE)

stopCluster(cl)

```


```{r}
# Test data
index <- createDataPartition(data_svm$Arrest, p = .03, list = TRUE)

test <- data_svm[index$Resample,]

# Confusion matrix
test$Arrest <- as.factor(test$Arrest)

conf_matrix1 <- confusionMatrix(data = predict(svm.model, newdata = test), reference = test$Arrest, positive = 'true', mode = "everything")

conf_matrix1
```

2. Downsampling : downsampling is a mechanism that reduces the count of training samples falling under the majority class, by randomly removing instances from the majority class until the class distribution is balanced. This method is straightforward but may discard potentially valuable information. We make sure that the model is trained using a random sample which is the same size as the sample used to fit the model before. 

```{r}
# Down sample the dataset
data_svm$Arrest = as.factor(data_svm$Arrest)

down_sample <- downSample(x = data_svm, y = data_svm$Arrest)

down_sample %>% count(Arrest)
```

```{r}
# Take random tarining data from down sampled data
index <- createDataPartition(down_sample$Arrest, p = nrow(train)/nrow(down_sample), list = TRUE)

train2 <- down_sample[index$Resample,] %>% select(-Class)

train2 %>% count(Arrest)

```

```{r}
# Fit SVM using down sampling
cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

svm.model2 <- train(Arrest ~ ., data = train2, trControl = control, method = 'svmRadial', family = binomial(), allowParallel = TRUE)

stopCluster(cl)

```

```{r}
# Test data
index <- createDataPartition(data_svm$Arrest, p = .03, list = TRUE)

test <- data_svm[index$Resample,]

# Confusion matrix
test$Arrest <- as.factor(test$Arrest)

conf_matrix2 <- confusionMatrix(data = predict(svm.model2, newdata = test), reference = test$Arrest, positive = 'true', mode = "everything")

conf_matrix2
```


3. Upsampling: this is a technique used in machine learning to address imbalanced datasets. Upsampling involves increasing the number of instances in the minority class to balance it with the number of instances in the majority class by randomly duplicating instances from the minority class until the class distribution is balanced. This method may lead to overfitting since it replicates existing instances. Again, we make sure that the model is trained using a random sample which is the same size as the samples used to fit the models before. 

```{r}
# Generate up sampled dataset
up_sample <- upSample(x = data_svm, y = data_svm$Arrest)

up_sample %>% count(Arrest)

# Up sampled training data
index <- createDataPartition(up_sample$Arrest, p = nrow(train)/nrow(up_sample), list = TRUE)

train3 <- up_sample[index$Resample,] %>% select(-Class)

train3 %>% count(Arrest)

```

```{r}
# Fit SVM with upsampling
cl <- makePSOCKcluster(detectCores() - 1)

registerDoParallel(cl)

svm.model3 <- train(Arrest ~ ., data = train3, trControl = control, method = 'svmRadial', family = binomial(), allowParallel = TRUE)

stopCluster(cl)

# Test data

index <- createDataPartition(data_svm$Arrest, p = .03, list = TRUE)

test <- data_svm[index$Resample,]

test$Arrest <- as.factor(test$Arrest)

# Confucion matrix
conf_matrix3 <- confusionMatrix(data = predict(svm.model3, newdata = test), reference = test$Arrest, positive = 'true', mode = "everything")

conf_matrix3

```


We want to present the results from all 3 models below. We plot the sensitivities, specificities and for a single, combined accuracy measure, the F1 scores, and their corresponding 95% confidence intervals. 

```{r}
D = conf_matrix1$table[1,1]
C = conf_matrix1$table[1,2]
B = conf_matrix1$table[2,1]
A = conf_matrix1$table[2,2]

accuracy_table <- data.frame() 

accuracy_table <- rbind(accuracy_table,cbind("model 1", "sens",BinomCI(A, A+C)))
accuracy_table <- rbind(accuracy_table,cbind("model 1", "spec",BinomCI(D, B+D)))
accuracy_table <- rbind(accuracy_table,cbind("model 1", "F1",BinomCI(A,A+B/2+C/2)))

D = conf_matrix2$table[1,1]
C = conf_matrix2$table[1,2]
B = conf_matrix2$table[2,1]
A = conf_matrix2$table[2,2]

accuracy_table <- rbind(accuracy_table,cbind("model 2", "sens",BinomCI(A, A+C)))
accuracy_table <- rbind(accuracy_table,cbind("model 2", "spec",BinomCI(D, B+D)))
accuracy_table <- rbind(accuracy_table,cbind("model 2", "F1",BinomCI(A,A+B/2+C/2)))

D = conf_matrix3$table[1,1]
C = conf_matrix3$table[1,2]
B = conf_matrix3$table[2,1]
A = conf_matrix3$table[2,2]

accuracy_table <- rbind(accuracy_table,cbind("model 3", "sens",BinomCI(A, A+C)))
accuracy_table <- rbind(accuracy_table,cbind("model 3", "spec",BinomCI(D, B+D)))
accuracy_table <- rbind(accuracy_table,cbind("model 3", "F1",BinomCI(A,A+B/2+C/2)))



colnames(accuracy_table) <- c("model", "type", "est", "lwr", "upr")

accuracy_table$est <- as.numeric(accuracy_table$est)
accuracy_table$lwr <- as.numeric(accuracy_table$lwr)
accuracy_table$upr <- as.numeric(accuracy_table$upr)

accuracy_table %>% group_by(type) %>% 
  ggplot(aes(x = type,y = est, color = type)) +
  geom_point() +
  geom_errorbar(aes(ymin = lwr, ymax = upr)) +
  facet_grid(cols = vars(model)) + 
  ylab("") + 
  xlab("Score type") +
  scale_y_continuous(n.breaks = 10) + 
  theme_minimal()
  
```

In the assessment of three SVM models on the Chicago crime dataset, Models 2 and 3 are meant to deal with the challenges of unbalanced training data using downsampling and upsampling. Model 2 achieves a sensitivity of 63.92% and a specificity of 84.08%, while Model 3 achieves very similar metrics with a sensitivity of 63.44% and a specificity of 84.06%. 

Conversely, Model 1 demonstrates outstanding specificity at 98.12% but at the cost of poor sensitivity (44.27%). This is due to the fact that with Model 1, we used a proportional sample where the negatives outweight the positives, so in this model negative outcomes dominate. The high specificity of this model suggests that it makes very few false positive predictions, but that is because it makes few positive predictions overall. 

The choice between these models depends on the cost considerations associated with false positives and false negatives. If the priority is to minimize false positives, especially when the cost is high, Model 1 is a strong candidate due to its exceptional specificity. Alternatively, if capturing arrests is crucial and the cost of false negatives is significant, Models 2 or 3 would be the preferred choice with their superior sensitivity.

Examining the F1 score, a metric that balances precision and recall, all three models exhibit nearly identical performance. Model 1 achieves an F1 score of 58.58%, Model 2 with 57.56%, and Model 3 with 57.24%. However, Models 2 and 3 show slightly narrower intervals for the F1 scores compared to Model 1, suggesting a higher degree of confidence in the F1 scores for Models 2 and 3. 

In conclusion, the implemented downsampling in Model 2 and upsampling in Model 3 have contributed to some improvements in addressing the challenges of unbalanced training data in the dataset. While these sampling techniques have led to more balanced sensitivities and specificities, the models' overall performance remains very similar to the performance of Model 1, where we used a proportional sample.


=======


## Predict Number of Cases withing Radius of 5 km

### Processing variable 'Date'

```{r}
data2 <- df %>% 
  select(Date, Primary.Type, Longitude, Latitude)

data2 %<>% 
  filter(!Primary.Type %in% c("NON-CRIMINAL", "OTHER NARCOTIC VIOLATION", "PUBLIC INDECENCY", "HUMAN TRAFFICKING", "OBSCENITY"))

data2 %<>% 
  mutate(Date = as.POSIXct(Date, format = "%m/%d/%Y %I:%M:%S %p"),
         month = format(as.Date(Date, format = "%m/%d/%Y"), "%m"))

data2 %<>%  
  na.omit()
```


```{r}
calculate_points_within_5km <- function(data, target_month, target_crime_type) {
   # Filter data based on specific inputs
  data %<>% filter(month == target_month) %>% 
    filter(Primary.Type == target_crime_type)

  for (i in 1:nrow(data)) {
  distances <- numeric(nrow(data))  # Initialize a vector to store distances for each point
  
  for (j in 1:nrow(data)) {
    # Calculate distances between point i and all other points
    distances[j] <- distGeo(c(data$Longitude[i], data$Latitude[i]),c(data$Longitude[j], data$Latitude[j]))
  }
  
  # Store the distances in the data frame
  data$points_within_5km[i] <- sum(distances <= 5000)
  }
  return(data)
}

```

# Split training and test sets

```{r}
data_Jan_Rob <- calculate_points_within_5km(data2, target_month = "01", target_crime_type = "ROBBERY")
```

```{r}
# Split the data into training and testing sets
set.seed(42)
split_index <- sample(seq_len(nrow(data_Jan_Rob)), size = 0.8 * nrow(data_Jan_Rob))
train_data <- data_Jan_Rob[split_index, ]
test_data <- data_Jan_Rob[-split_index, ]
```

### Linear Regression



```{r}
# Fit a linear regression model
reg_Jan_Rob <- lm(points_within_5km ~ Longitude + Latitude, data = train_data)

# Make predictions on the test set
pred_Jan_Rob <- predict(reg_Jan_Rob, newdata = test_data)
```

```{r}
# Evaluate the model
mse <- mean((test_data$points_within_5km - pred_Jan_Rob)^2)
mae <- mean(abs(test_data$points_within_5km - pred_Jan_Rob))
rmse <- sqrt(mean((test_data$points_within_5km - pred_Jan_Rob)^2))
rsquared <- 1 - (sum((test_data$points_within_5km - pred_Jan_Rob)^2) / sum((test_data$points_within_5km - mean(test_data$points_within_5km))^2))

print(paste("Mean Squared Error:", mse))
print(paste("Mean Absolute Error:", mae))
print(paste("Root Mean Squared Error:", rmse))
print(paste("R-squared:", rsquared))

```

```{r}
# check assumptions
par(mfrow = c(2, 2))
plot(reg_Jan_Rob)

# Plotting actual vs. predicted values
dev.new()
plot(test_data$points_within_5km, pred_Jan_Rob, main = "Actual vs. Predicted", xlab = "Actual", ylab = "Predicted")
abline(0, 1, col = "red")  # Adding a line of equality
```


### KNN clustering

```{r}
# Combine longitude and latitude into matrices
train_coordinates <- cbind(train_data$Longitude, train_data$Latitude)
test_coordinates <- cbind(test_data$Longitude, test_data$Latitude)

# Use KNN for regression to predict the number of crimes on the test set
predicted_crimes1 <- knn(train = train_coordinates, test = test_coordinates, cl = train_data$points_within_5km, k = 1)
# Evaluate the performance (you can use various regression metrics)
mse1 <- mean((as.numeric(predicted_crimes1) - test_data$points_within_5km)^2)
print(paste(" Mean Squared Error (MSE):", mse1))
```

```{r}
# Combine longitude and latitude into matrices
train_coordinates <- cbind(train_data$Longitude, train_data$Latitude)
test_coordinates <- cbind(test_data$Longitude, test_data$Latitude)

# Use KNN for regression to predict the number of crimes on the test set
predicted_crimes3 <- knn(train = train_coordinates, test = test_coordinates, cl = train_data$points_within_5km, k =3)
# Evaluate the performance (you can use various regression metrics)
mse3 <- mean((as.numeric(predicted_crimes3) - test_data$points_within_5km)^2)
print(paste(" Mean Squared Error (MSE):", mse3))
```

```{r}
# Set a range of k values
k_values <- c(1, 3, 5, 7, 9, 11, 13, 15)

# Initialize an empty vector to store MSE values
mse_values <- numeric(length(k_values))

# Loop through different k values
for (i in 1:length(k_values)) {
  # Use KNN for regression to predict the number of crimes on the test set
  predicted_crimes <- knn(train = train_coordinates, test = test_coordinates, cl = train_data$points_within_5km, k = k_values[i])
  
  # Evaluate the performance (you can use various regression metrics)
  mse_values[i] <- mean((as.numeric(predicted_crimes) - test_data$points_within_5km)^2)
}

# Plot the MSE values for different k
plot(k_values, mse_values, type = "b", main = "MSE for Different k Values", xlab = "k", ylab = "Mean Squared Error")

```

